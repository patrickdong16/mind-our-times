#!/usr/bin/env python3
"""
Podcast Enhanced Summary Generator (Option C)

Uses yt-dlp to download subtitles, then GPT-4o to generate
high-quality, structured summaries from full transcripts.

Cost: ~$0.04-0.15 per podcast (depending on length)
"""

import json
import os
import subprocess
import sys
import tempfile
from pathlib import Path
from typing import Optional, Dict, List

# Paths
SCRIPT_DIR = Path(__file__).parent
WORKSPACE = SCRIPT_DIR.parent  # mind-our-times/
ROOT_WORKSPACE = SCRIPT_DIR.parent.parent  # ~/.openclaw/workspace/
DATA_FILE = WORKSPACE / "podcast-friday/frontend/data.json"
OPENAI_KEY_FILE = ROOT_WORKSPACE / ".config/api_keys/openai"
YT_DLP = "yt-dlp"

def load_openai_key():
    return OPENAI_KEY_FILE.read_text().strip()

def download_transcript(video_id: str) -> Optional[str]:
    """Download transcript using youtube-transcript-api."""
    try:
        from youtube_transcript_api import YouTubeTranscriptApi
        
        api = YouTubeTranscriptApi()
        transcript = api.fetch(video_id)
        
        # Combine all snippets into full text
        text_parts = [snippet.text for snippet in transcript.snippets]
        full_text = ' '.join(text_parts)
        
        # Clean up
        full_text = full_text.replace('\n', ' ').replace('  ', ' ')
        
        print(f"  ğŸ“¥ Got transcript: {len(full_text):,} chars")
        return full_text
        
    except Exception as e:
        error_name = type(e).__name__
        if 'NoTranscript' in error_name or 'Disabled' in error_name:
            print(f"  âš ï¸ No transcript available for {video_id}")
        else:
            print(f"  âš ï¸ Transcript error ({error_name}): {str(e)[:50]}")
        return None

def parse_srt(content: str) -> str:
    """Parse SRT/VTT format, extract text, remove duplicates and timestamps."""
    import re
    
    lines = []
    seen = set()
    
    for line in content.split('\n'):
        line = line.strip()
        
        # Skip VTT header
        if line.startswith('WEBVTT') or line.startswith('Kind:') or line.startswith('Language:'):
            continue
        # Skip timestamp lines (both SRT and VTT format)
        if '-->' in line:
            continue
        # Skip numeric index lines (SRT)
        if line.isdigit():
            continue
        # Skip empty lines
        if not line:
            continue
        # Skip position/alignment tags
        if line.startswith('align:') or line.startswith('position:'):
            continue
        
        # Remove VTT tags like <c>, </c>, <00:00:00.000>
        line = re.sub(r'<[^>]+>', '', line)
        line = line.strip()
        
        if not line:
            continue
            
        # Skip duplicate lines (common in auto-subs)
        if line in seen:
            continue
        seen.add(line)
        lines.append(line)
    
    # Join and clean up
    text = ' '.join(lines)
    # Remove common artifacts
    text = text.replace('[Music]', '').replace('[Applause]', '')
    text = text.replace('  ', ' ')  # Clean double spaces
    return text

def generate_enhanced_summary(video: dict, transcript: Optional[str], max_retries: int = 3) -> Optional[dict]:
    """Generate enhanced summary using GPT-4o with full transcript."""
    import urllib.request
    import time
    
    api_key = load_openai_key()
    
    # Build context
    if transcript and len(transcript) > 500:
        # Use transcript (truncate if too long for context)
        max_chars = 100000  # ~75K tokens, safe for 128K context
        context = transcript[:max_chars]
        context_type = "full transcript"
    else:
        # Fallback to description
        context = video.get('description', '')[:3000]
        context_type = "description only"
    
    print(f"  ğŸ“ Using {context_type} ({len(context):,} chars)")
    
    # è·å–å‘å¸ƒæ—¥æœŸ
    published_at = video.get('publishedAt', '')
    if published_at:
        # æ ¼å¼åŒ–æ—¥æœŸï¼š2026-02-03T12:00:00Z -> 2026å¹´2æœˆ3æ—¥
        try:
            from datetime import datetime
            dt = datetime.fromisoformat(published_at.replace('Z', '+00:00'))
            date_str = f"{dt.year}å¹´{dt.month}æœˆ{dt.day}æ—¥"
        except:
            date_str = published_at[:10]
    else:
        date_str = ""
    
    prompt = f"""ä½ æ˜¯ä¸€ä½çŸ¥è¯†å¯†åº¦æé«˜çš„ä¸­æ–‡å†…å®¹ç­–å±•äººï¼Œä¸ºé«˜çŸ¥äººç¾¤ç­›é€‰æ·±åº¦å†…å®¹ã€‚

è¯·ä¸ºä»¥ä¸‹æ’­å®¢ç”Ÿæˆç»“æ„åŒ–çš„æ·±åº¦æ‘˜è¦ã€‚

ã€æ’­å®¢ä¿¡æ¯ã€‘
æ ‡é¢˜ï¼š{video.get('title', '')}
é¢‘é“ï¼š{video.get('channelName', '')}
æ—¶é•¿ï¼š{video.get('durationFormatted', '')}
å‘å¸ƒæ—¥æœŸï¼š{date_str}

ã€{context_type.upper()}ã€‘
{context}

ã€è¾“å‡ºè¦æ±‚ã€‘
è¯·è¿”å› JSON æ ¼å¼ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š

{{
  "intro": "å¼€ç¯‡å¯¼è¯­ï¼ˆ80-120å­—ï¼‰ï¼šèåˆæ ‡é¢˜ä¸»é¢˜ + ä¸ºä»€ä¹ˆå€¼å¾—å¬ + å‘å¸ƒæ—¶é—´ã€‚
    ä¾‹å¦‚ï¼š'{date_str}ï¼ŒLex Fridman ä¸ OpenAI è”åˆåˆ›å§‹äºº Ilya Sutskever å±•å¼€äº†ä¸€åœºå…³äº...'
    è¦æ±‚ï¼šè‡ªç„¶æµç•…ï¼Œäº¤ä»£æ¸…æ¥šè¿™æœŸçš„æ ¸å¿ƒä¸»é¢˜å’Œæœ‰æ„æ€çš„åœ°æ–¹ï¼Œä¸è¦'æ ‡é¢˜ï¼š'è¿™æ ·çš„å°æ ‡é¢˜",
  
  "summary_cn": "æ ¸å¿ƒå†…å®¹ï¼ˆ600-800å­—ï¼‰çš„æ·±åº¦æ‘˜è¦ã€‚è¦æ±‚ï¼š
    - è‡ªç„¶åˆ†æ®µï¼Œä¸è¦ç”¨'æ ¸å¿ƒè®ºç‚¹ï¼š''å…³é”®è®ºæ®ï¼š'è¿™æ ·çš„å°æ ‡é¢˜
    - ç¬¬ä¸€æ®µï¼šè¿™æœŸæ’­å®¢æœ€é‡è¦çš„æ´å¯Ÿæ˜¯ä»€ä¹ˆï¼Ÿ
    - ç¬¬äºŒæ®µï¼šæ”¯æ’‘æ ¸å¿ƒè®ºç‚¹çš„2-3ä¸ªå…³é”®äº‹å®æˆ–è®ºè¯ï¼Œå†™æ¸…æ¥šè®ºç‚¹å’Œè®ºæ®çš„é€»è¾‘å…³ç³»
    - ç¬¬ä¸‰æ®µï¼šäº‰è®®ã€å¯ç¤ºã€æˆ–å€¼å¾—æ·±æ€çš„é—®é¢˜
    è¯­è¨€é£æ ¼ï¼šã€Šç»æµå­¦äººã€‹ä¸­æ–‡ç‰ˆï¼Œä¿¡æ¯å¯†é›†ï¼Œé¿å…ç©ºè¯",
  
  "key_quotes": [
    {{
      "en": "åŸæ–‡é‡‘å¥1ï¼ˆè‹±æ–‡åŸè¯ï¼Œä»transcriptä¸­æå–æœ€æœ‰å†²å‡»åŠ›çš„è¡¨è¾¾ï¼‰",
      "cn": "ä¸­æ–‡ç¿»è¯‘1"
    }},
    {{
      "en": "åŸæ–‡é‡‘å¥2",
      "cn": "ä¸­æ–‡ç¿»è¯‘2"
    }}
  ],
  
  "guest_bio": "å˜‰å®¾æ·±åº¦ä»‹ç»ï¼ˆ150-200å­—ï¼‰ï¼šåŒ…æ‹¬å­¦æœ¯/èŒä¸šèƒŒæ™¯ã€ä»£è¡¨ä½œå“æˆ–æˆå°±ã€ç‹¬ç‰¹è§†è§’æ¥æºã€ä¸ºä½•æ˜¯è¿™ä¸ªè¯é¢˜çš„æƒå¨å£°éŸ³ã€‚å¦‚æœæ˜¯å¤šä½å˜‰å®¾ï¼Œåˆ†åˆ«ä»‹ç»ã€‚å¦‚æœæ˜¯ä¸»æŒäººç‹¬ç™½ï¼Œå†™ä¸»æŒäººçš„èƒŒæ™¯å’Œå½±å“åŠ›ã€‚",
  
  "title_cn": "ä¸­æ–‡æ ‡é¢˜ï¼ˆç®€æ´æœ‰åŠ›ï¼Œä¸è¶…è¿‡25å­—ï¼‰"
}}

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""

    body = {
        "model": "gpt-4o",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.7,
        "response_format": {"type": "json_object"}
    }
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    # Retry with exponential backoff
    for attempt in range(max_retries):
        req = urllib.request.Request(
            "https://api.openai.com/v1/chat/completions",
            data=json.dumps(body).encode('utf-8'),
            headers=headers,
            method='POST'
        )
        
        try:
            with urllib.request.urlopen(req, timeout=180) as resp:
                result = json.loads(resp.read().decode('utf-8'))
                content = result['choices'][0]['message']['content']
                return json.loads(content)
        except urllib.error.HTTPError as e:
            if e.code == 429:  # Rate limit
                wait_time = (2 ** attempt) * 15  # 15s, 30s, 60s
                print(f"  â³ Rate limited, waiting {wait_time}s (attempt {attempt + 1}/{max_retries})")
                time.sleep(wait_time)
                continue
            else:
                print(f"  âŒ GPT-4o HTTP error {e.code}: {e.reason}")
                return None
        except Exception as e:
            print(f"  âŒ GPT-4o error: {e}")
            return None
    
    print(f"  âŒ Failed after {max_retries} retries")
    return None

def process_podcasts():
    """Process all podcasts in data.json with enhanced summaries."""
    
    if not DATA_FILE.exists():
        print(f"âŒ Data file not found: {DATA_FILE}")
        sys.exit(1)
    
    data = json.loads(DATA_FILE.read_text())
    episodes = data.get('episodes', [])
    
    print(f"ğŸ“¡ Processing {len(episodes)} podcasts with enhanced summaries...\n")
    
    for i, ep in enumerate(episodes, 1):
        video_id = ep.get('videoId', '')
        title = ep.get('title', '')[:50]
        print(f"[{i}/{len(episodes)}] {title}...")
        
        # Download transcript
        transcript = download_transcript(video_id)
        
        # Generate enhanced summary
        enhanced = generate_enhanced_summary(ep, transcript)
        
        if enhanced:
            # Update episode with enhanced data
            ep['intro'] = enhanced.get('intro', '')  # å¼€ç¯‡å¯¼è¯­ï¼ˆèåˆæ ‡é¢˜+ä¸ºä»€ä¹ˆå€¼å¾—å¬+æ—¶é—´ï¼‰
            ep['summary_cn'] = enhanced.get('summary_cn', ep.get('summary_cn', ''))
            ep['why_listen'] = enhanced.get('why_listen', ep.get('why_listen', ''))
            ep['key_quotes'] = enhanced.get('key_quotes', [])
            ep['guest_bio'] = enhanced.get('guest_bio', '')
            ep['title_cn'] = enhanced.get('title_cn', ep.get('title_cn', ''))
            print(f"  âœ… Enhanced ({len(ep['summary_cn'])} chars)")
        else:
            print(f"  âš ï¸ Keeping original summary")
        
        print()
        
        # Delay between podcasts to avoid rate limits
        if i < len(episodes):
            import time
            time.sleep(5)
    
    # Save updated data
    DATA_FILE.write_text(json.dumps(data, ensure_ascii=False, indent=2))
    print(f"ğŸ’¾ Saved to {DATA_FILE}")
    
    # Show summary
    print("\nğŸ“Š Summary:")
    for ep in episodes:
        summary_len = len(ep.get('summary_cn', ''))
        has_quotes = 'âœ“' if ep.get('key_quotes') else 'âœ—'
        print(f"  [{ep.get('domain', '?')}] {ep.get('title_cn', '')[:30]}... ({summary_len} chars, quotes: {has_quotes})")

if __name__ == "__main__":
    process_podcasts()
